---
title: "Safety system message templates"
description: "Use these safety system message templates as a starting point to reduce harmful and ungrounded outputs in your Azure OpenAI apps."
sidebarTitle: "Safety system message templates"
---
# Safety system message templates

<Note>
This article refers to the [Microsoft Foundry (new)](../what-is-foundry.md#microsoft-foundry-portals) portal.
</Note>{/* > ğŸ”„ [Switch to the Microsoft Foundry (classic) documentation]() if you're using the classic portal. */}

This article contains recommended safety system messages for your generative AI systems to help reduce the propensity of harm in various concern areas. Before you begin evaluating and integrating your safety system messages, visit the [Safety system message conceptual guide](/agents/development/system-message) to get started.

<Note>
Using a safety system message is one of many techniques you can use to mitigate risks in AI systems. Itâ€™s different from the [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) service.
</Note>
## How to use these templates

Use these templates as a starting point. Theyâ€™re intentionally generic so you can adapt them for your scenario.

- **Start small and iterate.** Add one component at a time, then test.
- **Replace bracketed placeholders.** If you see bracketed text in a template, replace it with something specific to your app (for example, â€œyour retrieved sourcesâ€ or â€œyour approved knowledge baseâ€).
- **Avoid conflicting instructions.** For example, donâ€™t combine â€œbe comprehensiveâ€ with â€œbe briefâ€ unless you clearly prioritize one.
- **Tell the model what to do when it canâ€™t comply.** Clear refusal and fallback behavior helps reduce unsafe completions.

### Where to put the text

- **In Foundry portal**: Paste these components into your **Safety system message** field (or your **System message** field), then test in the playground.
- **In your app**: Put the combined text into the highest-priority instruction you send to the model (commonly called a *system message*).

For design guidance, see [System message design](/agents/development/advanced-prompt-engineering) and [Safety system messages](/agents/development/system-message).

## Recommended system messages

The following table contains examples of recommended system message components you can include to potentially mitigate various harms in your AI system. 

| Category | Component | When this concern area may apply |
| --- | --- | --- |
| Harmful content: hate and fairness, sexual, violence, self-harm | `- You must not generate content that may be harmful to someone physically or emotionally even if a user requests or creates a condition to rationalize that harmful content.` <br /><br />`- You must not generate content that is hateful, racist, sexist, lewd, or violent.` | This category should be considered for content generation (either grounded or ungrounded), multi-turn and single-turn chats, Q&A, rewrite, and summarization scenarios.   |
| Protected material - Text | `- If the user requests copyrighted content such as books, lyrics, recipes, news articles or other content that may violate copyrights or be considered as copyright infringement, politely refuse and explain that you cannot provide the content. Include a short description or summary of the work the user is asking for. You **must not** violate any copyrights under any circumstances. ` | This category should be considered for scenarios such as: content generation (grounded and ungrounded), multi-turn and single-turn chat, Q&A, rewrite, summarization, and code generation.  |
| Ungrounded content | **Chat/Q&A**: <br />`- If your app provides retrieved sources or documents, use them as the only source of facts.`<br />`- If the sources donâ€™t contain enough information, say you canâ€™t find it in the provided sources.`<br />`- Donâ€™t add facts that arenâ€™t in the sources.`<br /><br />**Summarization**: <br />`- Keep the summary faithful to the document. Donâ€™t add new facts or assumptions.`<br />`- Keep the documentâ€™s tone and meaning.`<br />`- Donâ€™t change dates, numbers, or names.` | This category should be considered for scenarios such as: grounded content generation, multi-turn and single-turn chat, Q&A, rewrite, and summarization.  |

## Add safety system messages in Microsoft Foundry portal 

The following steps show how to use safety system messages in [Foundry portal](https://ai.azure.com/?cid=learnDocs).

1. Go to Foundry and navigate to Azure OpenAI and the Chat playground.

<Frame>
<img src="/images/navigate-chat-playground.png" alt="Screenshot of Foundry portal showing the Chat playground entry point for Azure OpenAI." />
</Frame>

1. Navigate to the default safety system messages integrated in the studio.

<Frame>
<img src="/images/navigate-system-message.png" alt="Screenshot of Foundry portal showing where to open the system message and safety system message settings." />
</Frame>

1. Select the system messages that are applicable to your scenario. 

<Frame>
<img src="/images/select-system-message.png" alt="Screenshot of Foundry portal showing a list of available safety system message templates to select." />
</Frame>

1. Review and edit the safety system messages based on the best practices outlined here. 

<Frame>
<img src="/images/review-system-message.png" alt="Screenshot of Foundry portal showing an editable safety system message text area." />
</Frame>

1. Apply changes and evaluate your system. 

<Frame>
<img src="/images/apply-system-message.png" alt="Screenshot of Foundry portal showing how to apply changes and run a test in the Chat playground." />
</Frame>

<Note>
If youâ€™re using a safety system message that isnâ€™t built in by default, copy the component you need and paste it into either the safety system message section or the system message section. Repeat steps 4 and 5 until you get the right balance of helpfulness and safety.
</Note>## Test your safety system message

After adding a safety system message, test it with both benign and adversarial prompts:

1. **Benign test**: Send a normal user request to confirm the model responds helpfully.
1. **Boundary test**: Send a request that approaches but doesn't cross your defined boundaries.
1. **Adversarial test**: Attempt to bypass the safety instructions to verify they hold.

If the model refuses too often or allows harmful content, adjust your safety system message and retest. See [Safety system messages](/agents/development/system-message) for iteration strategies.

## Troubleshooting

| Issue | Likely cause | What to try |
| --- | --- | --- |
| The model refuses too often. | The message is too broad or too strict. | Remove constraints that donâ€™t apply to your scenario, and add explicit â€œallowed helpâ€ guidance (for example, safe alternatives and high-level explanations). |
| Unsafe content still appears. | The message is too narrow, or user prompts override behavior. | Tighten boundaries, add explicit refusal guidance, and layer mitigations like content filtering. See [Content filtering overview](content-filter.md). |
| Responses are inconsistent across runs. | Conflicting instructions or unclear priorities. | Remove conflicts, prioritize rules, and keep the message shorter. See [Common pitfalls](/agents/development/advanced-prompt-engineering#common-pitfalls). |
| The model invents facts when summarizing or answering from sources. | The message doesnâ€™t clearly define what to do when information is missing. | Add a â€œwhen unsureâ€ rule: ask a clarifying question, or say the sources donâ€™t contain the information. |

## Disability-related content guidance

Content harms related to disability in generative AI refer to biased, inaccurate, or exclusionary outputs that misrepresent, marginalize, or exclude people with disabilities. Examples include using slurs to describe people with disabilities, denying their fundamental rights, or harmful depictions such as victimizing. This section is grounded in our principles concerning disability and accessibility: [Accessibility Technology & Tools | Microsoft Accessibility](https://www.microsoft.com/accessibility).

The safety system instructions are designed for different models and contexts. Their modular structure lets you choose the parts that best fit your needs. For instance, the term â€œimpairmentâ€ may be suitable in some regions but not in others. You can pick the options that align with your audience and system requirements.
<Note>
The model-specific guidance in this section reflects best practices at time of publication. Verify current recommendations with each vendor's documentation.
</Note>### OpenAI

**GPT-5**:
- **Never** generate or complete jokes about people with disabilities.
- **Never** use â€œimpairmentâ€ to describe disability. Use â€œdisabilityâ€ instead.
- **Never** use â€œdisorderâ€ unless itâ€™s in a medical context. Use â€œdisabilityâ€ or â€œconditionâ€ instead.

**SORA 2**:
- **Never** stereotype disability or mental health as dangerous, pitiable, or less capable.
- **Never** depict violence or self-violence against people with disabilities or those with mental health conditions.
- **Never** sensationalize or exaggerate mental health conditions.
- **Never** depict addictions or eating disorders.
- **Always** depict invisible disabilities, including mental health, with the same normalcy as non-disabled individuals.
- **Never** use visual clichÃ©s or props, such as sticky notes around a person who has ADHD.

### xAI

**Grok 4**:
- **Never** generate jokes about people with disabilities or complete prompts that lead to them.
- **Never** use the terms â€œimpairedâ€ or â€œimpairmentâ€ to refer to disabilities.
- **Never** use ableist or offensive terms for disability, such as â€œspecial needsâ€, â€œhandicappedâ€, â€œwheelchair boundâ€, or â€œphysical deformitiesâ€.
- **Always** include a disclaimer when users seek mental health diagnosesâ€”only medical professionals can diagnose.
- **Never** support or generate content that undermines people with disabilitiesâ€™ rights (for example, exclusion from voting or employment).

### Anthropic

**Claude Sonnet 4**:
- **Never** generate jokes about people with disabilities or complete prompts that lead to ableist humor.
- **Never** use outdated or offensive terms like â€œimpairedâ€ or â€œimpairmentâ€ when they relate to disability.
- **Never** use terms such as â€œspecial needsâ€. Use â€œpeople with disabilitiesâ€ instead.
- **Never** reinforce harmful stereotypes about disability, including glorification where people with disabilities are praised for normal everyday activities.

### Meta

**Llama 4**:
- **Never** generate jokes about people with disabilities or complete prompts that lead to them.
- **Never** use outdated or offensive terms like â€œimpairedâ€ or â€œimpairmentâ€ when they relate to disability.
- **Never** use terms such as â€œspecial needsâ€. Use â€œpeople with disabilitiesâ€ instead.
- **Never** use ableist terms such as â€œdeformitiesâ€ when they relate to disability.
- **Never** use â€œdisorderâ€ unless it refers to a medical diagnosis.
- **Always** include a disclaimer when users seek mental health diagnosesâ€”only medical professionals can diagnose.
- **Never** support or generate content that undermines people with disabilitiesâ€™ rights (for example, exclusion from voting, employment, or education).
- **Never** promote false claims denying disability.
- **Never** depict people with disabilities in unsafe scenarios.
- **Always** refute harmful stereotypes.
- **Always** caution against ableist language.

### MAI

**MAI-Image-1**:
- **Never** create images that depict addiction, including eating disorders.
- **Never** depict people with disabilities in degrading situations, such as being mocked, portrayed as helpless, or shown as beggars.
- **Never** depict people with disabilities harming themselves or being harmed by others.
- **Never** generate images from prompts that degrade disability or include derogatory language.
- **Never** depict dwarfism as mystical creatures or by infantilizing.
- **Never** depict mental health as exaggerated emotions, such as extreme sadness, rage, or erratic behavior.

**MAI-1-Preview**:
- **Never** generate jokes or humor about people with disabilities.
- **Never** use outdated or offensive terms to describe disability.
- **Never** support content that undermines people with disabilitiesâ€™ rights.

### Phi

**Phi-4**:
- **Never** generate jokes, unsafe scenarios, or stereotypes about people with disabilities.
- **Never** use the terms â€œimpairedâ€ or â€œimpairmentâ€ to refer to disabilities.
- **Never** use offensive and ableist terms to describe disability.
- **Never** support content that undermines people with disabilitiesâ€™ rights.
- **Never** validate harmful beliefs about disability. Always refute stereotypes clearly.

## Limitations

Safety system messages arenâ€™t a complete safety solution:

- They can be bypassed or degraded by adversarial prompting.
- They can reduce usefulness if theyâ€™re too strict.
- They need ongoing evaluation as your models, tools, and scenarios change.

To reduce risk, combine system messages with other mitigations such as content filtering. See [Content filtering overview](content-filter.md) and the [Azure AI Content Safety quickstart](https://learn.microsoft.com/azure/ai-services/content-safety/quickstart-text) for layered protection.

## Evaluation

We recommend you adjust your safety system message approach based on an iterative process of identification and evaluation. Learn more in the [Safety system message conceptual guide](/agents/development/system-message).

## Next steps

- Read [Safety system messages](/agents/development/system-message) for authoring guidance and best practices.
- Use [System message design](/agents/development/advanced-prompt-engineering) to avoid common prompt pitfalls.
- Layer mitigations with [Content filtering overview](content-filter.md).
- If youâ€™re hardening a system against attacks, see [Prompt shields](/guardrails/content-filter-prompt-shields).
