---
title: "How to generate text responses with Microsoft Foundry Models"
description: "Learn how to generate text responses from Foundry Models, such as Microsoft AI and DeepSeek models, by using the Responses API."
sidebarTitle: "How to generate text responses with Microsoft Foundry Models"
---
# How to generate text responses with Microsoft Foundry Models

<Note>
This article refers to the [Microsoft Foundry (new)](../what-is-foundry.md#microsoft-foundry-portals) portal.
</Note><!-- > ðŸ”„ [Switch to the Microsoft Foundry (classic) documentation]() if you're using the classic portal. -->

This article explains how to generate text responses for Foundry Models, such as Microsoft AI, DeepSeek, and Grok models, by using the Responses API. For a full list of the Foundry Models that support use of the Responses API, see [Supported Foundry Models](#supported-foundry-models). 

## Prerequisites

To use the Responses API with deployed models in your application, you need:

- An Azure subscription. If you're using [GitHub Models](https://docs.github.com/en/github-models/), you can upgrade your experience and create an Azure subscription in the process. Read [Upgrade from GitHub Models to Microsoft Foundry Models](/models/capabilities/quickstart-github-models) if that's your case.

- A Foundry project. This kind of project is managed under a Foundry resource. If you don't have a Foundry project, see [Create a project for Microsoft Foundry](/setup/create-projects).

- Your Foundry project's endpoint URL, which is of the form `https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME`.

- A deployment of a Foundry Model, such as the `MAI-DS-R1` model used in this article. If you don't have a deployment already, see [Add and configure Foundry Models](/models/capabilities/create-model-deployments) to a model deployment to your resource.

## Use the Responses API to generate text

Use the code in this section to make Responses API calls for Foundry Models. In the code samples, you create the client to consume the model and then send it a basic request. 

<Info>
Some of the SDK packages used in these samples are currently in preview. API surface and behavior might change before general availability.
</Info>
<Tip>
When you deploy a model in the Foundry portal, you assign it a deployment name. Use this deployment name (not the model catalog ID) in the `model` parameter of your API calls.
</Tip>
<Note>
Use keyless authentication with **Microsoft Entra ID**. To learn more about keyless authentication, see [What is Microsoft Entra authentication?](/entra/identity/authentication/overview-authentication) and [DefaultAzureCredential](https://learn.microsoft.com/azure/developer/python/sdk/authentication/overview#defaultazurecredential).
</Note>

<Tabs>
  <Tab title="Python">

    1. Install libraries, including the Azure Identity client library:

        ```bash
        pip install azure-identity
        pip install openai
        pip install --pre azure-ai-projects>=2.0.0b1 
        ```

    1. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses. 

        ```python
        import os
        from azure.identity import DefaultAzureCredential
        from azure.ai.projects import AIProjectClient

        project_client = AIProjectClient(
            endpoint="https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME",
            credential=DefaultAzureCredential(),
        )

        openai_client = project_client.get_openai_client()

        response = openai_client.responses.create(
            model="MAI-DS-R1", # Replace with your deployment name, not the model ID 
            input="What is the capital/major city of France?",
        )

        print(response.model_dump_json(indent=2)) 
        ```

  </Tab>
  <Tab title="C#">

    1. Install the Azure Identity client library:

        ```dotnetcli
        dotnet add package Azure.Identity
        ```

    1. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses. 

        ```csharp
        using Azure.Identity;
        using Azure.AI.Projects; 
        using Azure.AI.Projects.OpenAI;
        using OpenAI.Responses;

        #pragma warning disable OPENAI001

        const string deploymentName = "MAIDSR1"; // Replace with your deployment name, not the model ID 
        const string endpoint = "https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME";

        AIProjectClient projectClient = new(new Uri(endpoint), new DefaultAzureCredential());

        ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForModel(deploymentName);

        ResponseResult response = responseClient.CreateResponse("What is the capital/major city of France?");

        Console.WriteLine($"[ASSISTANT]: {response.GetOutputText()}");
        ```

  </Tab>
  <Tab title="JavaScript">

    [JavaScript v1 examples](/models/capabilities/supported-languages)

    1. Install the Azure Identity client library before you can use DefaultAzureCredential:

        ```bash
        npm install @azure/identity
        ```

    1. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses.  

        ```javascript
        import { AIProjectClient } from "@azure/ai-projects";
        import { DefaultAzureCredential } from "@azure/identity";

        const endpoint = "https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME";
        const deploymentName = "MAI-DS-R1"; // Replace with your deployment name, not the model ID 

        async function main() {
            const projectClient = new AIProjectClient(endpoint, new DefaultAzureCredential());
            const openAIClient = await projectClient.getOpenAIClient();

            const response = await openAIClient.responses.create({
                model: deploymentName,
                input: "What is the capital/major city of France?"
            });
            console.log(response.output_text);
        }

        main();
        ```

  </Tab>
  <Tab title="Java">

    Authentication with Microsoft Entra ID requires some initial setup. First, install the Azure Identity client library. For more options on how to install this library, see [Azure Identity client library for Java](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/identity/azure-identity/README.md#include-the-package).

    1. Add the Azure Identity client library:

        ```xml
        <dependency>
            <groupId>com.azure</groupId>
            <artifactId>azure-identity</artifactId>
            <version>1.18.0</version>
        </dependency>
        ```

        After setup, choose which type of credential from `azure.identity` to use. For example, use `DefaultAzureCredential` to authenticate the client. Authentication is easiest with `DefaultAzureCredential`, as it finds the best credential to use in its running environment.

    1. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses.  

        ```java
        import com.azure.ai.agents;
        import com.azure.core.util.Configuration;
        import com.azure.identity.DefaultAzureCredentialBuilder;
        import com.openai.models.responses.Response;
        import com.openai.models.responses.ResponseCreateParams;

        public class Sample {
            public static void main(String[] args) {
                String endpoint = "https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME";
                String deploymentName = "MAI-DS-R1"; // Replace with your deployment name, not the model ID

                ResponsesClient responsesClient = new AgentsClientBuilder()
                        .credential(new DefaultAzureCredentialBuilder().build())
                        .endpoint(endpoint)
                        .serviceVersion(AgentsServiceVersion.V2025_11_15_PREVIEW)
                        .buildResponsesClient();

                ResponseCreateParams responseRequest = new ResponseCreateParams.Builder()
                        .input("What is the capital/major city of France?")
                        .model(deploymentName)
                        .build();

                Response response = responsesClient.getResponseService().create(responseRequest);
            }
        }
       ```

  </Tab>
  <Tab title="REST">

    ```bash
    curl -X POST https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME/openai/responses?api-version={{API_VERSION}} \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $AZURE_OPENAI_AUTH_TOKEN" \
    -d '{
        "model": "MAI-DS-R1",
        "input": "What is the capital/major city of France?"
    }'
    ```

  </Tab>
</Tabs>

The response includes the generated text along with model and usage metadata.

## Supported Foundry Models

A selection of Foundry Models are supported for use with the Responses API.

### View supported models in the Foundry portal

<!-- View a list of models that are supported in Agents Service and supported for use with the Responses API. -->

To see a full list of the supported models in the Foundry portal:

Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

<img src="/images/new-foundry.png" />

1. Go to the **Model catalog**.
1. Filter the models by **Capabilities** and select **Agent supported**.

::: moniker-end

1. 

::: moniker range="foundry"
Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

<img src="/images/new-foundry.png" />

1. Select **Discover** in the upper-right navigation, then **Models** in the left pane.
1. Open the **Capabilities** dropdown and select the **Agent supported** filter.

::: moniker-end

### List of supported models

This section lists some of the Foundry Models that are supported for use with the Responses API. For the Azure OpenAI models that are supported, see [Available Azure OpenAI models](../../agents/concepts/model-region-support.md#available-models).

**Models sold directly by Azure:**

- **MAI-DS-R1**: Deterministic, precision-focused reasoning.
- **grok-4**: Frontier-scale reasoning for complex, multiple-step problem solving.
- **grok-4-fast-reasoning**: Accelerated agentic reasoning optimized for workflow automation.
- **grok-4-fast-non-reasoning**: High-throughput, low-latency generation and system routing.
- **grok-3**: Strong reasoning for complex, system-level workflows.
- **grok-3-mini**: Lightweight model optimized for interactive, high-volume use cases.
- **Llama-3.3-70B-Instruct**: Versatile model for enterprise Q&A, decision support, and system orchestration.
- **Llama-4-Maverick-17B-128E-Instruct-FP8**: FP8-optimized model that delivers fast, cost-efficient inference.
- **DeepSeek-V3-0324**: Multimodal understanding across text and images.
- **DeepSeek-V3.1**: Enhanced multimodal reasoning and grounded retrieval.
- **DeepSeek-R1-0528**: Advanced long-form and multiple-step reasoning.
- **gpt-oss-120b**: Open-ecosystem model that supports transparency and reproducibility.

## Troubleshoot common errors

| Error | Cause | Resolution |
|---|---|---|
| 401 Unauthorized | Invalid or expired credential | Verify your `DefaultAzureCredential` has the **Cognitive Services OpenAI User** role assigned on the resource. |
| 404 Not Found | Wrong endpoint or deployment name | Confirm your endpoint URL includes `/api/projects/YOUR_PROJECT_NAME` and the deployment name matches your Foundry portal. |
| 400 Model not supported | Model doesn't support Responses API | Check the [supported models list](#supported-foundry-models) and verify your deployment uses a compatible model. |

## Related content

- [Migrate from Azure AI Inference SDK to OpenAI SDK](../../how-to/model-inference-to-openai-migration.md)
- [Azure OpenAI supported programming languages](/models/capabilities/supported-languages)
- [Switch between OpenAI and Azure OpenAI endpoints](https://learn.microsoft.com/azure/developer/ai/how-to/switching-endpoints)
- [Model support for v1 Azure OpenAI API](/api-sdk/api-version-lifecycle#model-support)
