---
title: "How to migrate from Azure AI Inference SDK to OpenAI SDK"
description: "Learn how to migrate to OpenAI SDK from Azure AI Inference SDK for enhanced compatibility and unified APIs when working with Microsoft Foundry Models."
---
# Migrate from Azure AI Inference SDK to OpenAI SDK
This article provides guidance on migrating your applications from the Azure AI Inference SDK to the OpenAI SDK. The OpenAI SDK offers broader compatibility, access to the latest OpenAI features, and simplified code with unified patterns across Azure OpenAI and Foundry Models.

<Note>
The OpenAI SDK refers to the client libraries (such as the Python `openai` package or JavaScript `openai` npm package) that connect to [OpenAI v1 API endpoints](/api-sdk/api-version-lifecycle#api-evolution). These SDKs have their own versioning separate from the API version - for example, the Go OpenAI SDK is currently at v3, but it still connects to the OpenAI v1 API endpoints with `/openai/v1/` in the URL path.
</Note>

## Benefits of migrating

Migrating to the OpenAI SDK provides several advantages:

- **Broader model support**: Works with Azure OpenAI in Foundry Models and other Foundry Models from providers like DeepSeek and Grok
- **Unified API**: Uses the same SDK libraries and clients for both OpenAI and Azure OpenAI endpoints
- **Latest features**: Access to the newest OpenAI features without waiting for Azure-specific updates
- **Simplified authentication**: Built-in support for both API key and Microsoft Entra ID authentication
- **Implicit API versioning**: The v1 API eliminates the need to frequently update `api-version` parameters

## Key differences

The following table shows the main differences between the two SDKs:

| Aspect | Azure AI Inference SDK | OpenAI SDK |
|--------|------------------------|-------------|
| Client class | `ChatCompletionsClient` | `OpenAI` |
| Endpoint format | `https://<resource>.services.ai.azure.com/models` | `https://<resource>.openai.azure.com/openai/v1/` |
| API version | Required in URL or parameter | Not required (uses v1 API) |
| Model parameter | Optional (for multi-model endpoints) | Required (deployment name) |
| Authentication | Azure credentials only | API key or Azure credentials |

## Setup

Install the OpenAI SDK:

```bash
pip install openai
```

For Microsoft Entra ID authentication, also install:

```bash
pip install azure-identity
```

## Client configuration

<Tabs>
  <Tab title="OpenAI SDK">

    With API key authentication:

    ```python
    import os
    from openai import OpenAI

    client = OpenAI(
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        base_url="https://<resource>.openai.azure.com/openai/v1/",
    )
    ```

  </Tab>
  <Tab title="Azure AI Inference SDK">

    ```python
    import os
    from azure.ai.inference import ChatCompletionsClient
    from azure.core.credentials import AzureKeyCredential

    client = ChatCompletionsClient(
        endpoint="https://<resource>.services.ai.azure.com/models",
        credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
    )
    ```

  </Tab>
</Tabs>

With Microsoft Entra ID authentication:

<CodeGroup>
```python OpenAI SDK
    from openai import OpenAI
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    token_provider = get_bearer_token_provider(
        DefaultAzureCredential(), 
        "https://cognitiveservices.azure.com/.default"
    )

    client = OpenAI(
        base_url="https://<resource>.openai.azure.com/openai/v1/",
        api_key=token_provider,
    )
    ```

```python Azure AI Inference SDK
    from azure.ai.inference import ChatCompletionsClient
    from azure.identity import DefaultAzureCredential

    client = ChatCompletionsClient(
        endpoint="https://<resource>.services.ai.azure.com/models",
        credential=DefaultAzureCredential(),
        credential_scopes=["https://cognitiveservices.azure.com/.default"],
    )
    ```
</CodeGroup>

## Chat completions

<CodeGroup>
```python OpenAI SDK
    completion = client.chat.completions.create(
        model="DeepSeek-V3.1",  # Required: your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is Azure AI?"}
        ]
    )

    print(completion.choices[0].message.content)
    ```

```python Azure AI Inference SDK
    from azure.ai.inference.models import SystemMessage, UserMessage

    response = client.complete(
        messages=[
            SystemMessage(content="You are a helpful assistant."),
            UserMessage(content="What is Azure AI?"),
        ],
        model="DeepSeek-V3.1"  # Optional for single-model endpoints
    )

    print(response.choices[0].message.content)
    ```
</CodeGroup>

### Streaming

<CodeGroup>
```python OpenAI SDK
    stream = client.chat.completions.create(
        model="DeepSeek-V3.1",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Write a poem about Azure."}
        ],
        stream=True
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="")
    ```

```python Azure AI Inference SDK
    from azure.ai.inference.models import SystemMessage, UserMessage

    response = client.complete(
        stream=True,
        messages=[
            SystemMessage(content="You are a helpful assistant."),
            UserMessage(content="Write a poem about Azure."),
        ],
        model="DeepSeek-V3.1"
    )

    for update in response:
        if update.choices:
            print(update.choices[0].delta.content or "", end="")
    ```
</CodeGroup>

## Embeddings

<CodeGroup>
```python OpenAI SDK
    from openai import OpenAI
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    token_provider = get_bearer_token_provider(DefaultAzureCredential(), 
    "https://cognitiveservices.azure.com/.default")

    client = OpenAI(
        base_url = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
        api_key = token_provider,
    )

    response = client.embeddings.create(
        input = "How do I use Python in VS Code?",
        model = "text-embedding-3-large" // Use the name of your deployment
    )
    print(response.data[0].embedding)
    ```

```python Azure AI Inference SDK
    from azure.ai.inference import EmbeddingsClient
    from azure.core.credentials import AzureKeyCredential

    client = EmbeddingsClient(
        endpoint="https://<resource>.services.ai.azure.com/models",
        credential=AzureKeyCredential(os.environ["AZURE_INFERENCE_CREDENTIAL"]),
    )

    response = client.embed(
        input=["Your text string goes here"],
        model="text-embedding-3-small"
    )

    embedding = response.data[0].embedding
    ```
</CodeGroup>

## Setup

Install the OpenAI SDK:

```dotnetcli
dotnet add package OpenAI
```

For Microsoft Entra ID authentication, also install:

```dotnetcli
dotnet add package Azure.Identity
```

## Client configuration

With API key authentication:

<CodeGroup>
```csharp OpenAI SDK
    using OpenAI;
    using OpenAI.Chat;
    using System.ClientModel;

    ChatClient client = new(
        model: "gpt-4o-mini", // Your deployment name
        credential: new ApiKeyCredential(Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY")),
        options: new OpenAIClientOptions() { 
            Endpoint = new Uri("https://<resource>.openai.azure.com/openai/v1/")
        }
    );
    ```

```csharp Azure AI Inference SDK
    using Azure;
    using Azure.AI.Inference;

    ChatCompletionsClient client = new ChatCompletionsClient(
        new Uri("https://<resource>.services.ai.azure.com/models"),
        new AzureKeyCredential(Environment.GetEnvironmentVariable("AZURE_INFERENCE_CREDENTIAL"))
    );
    ```
</CodeGroup>

With Microsoft Entra ID authentication:

<CodeGroup>
```csharp OpenAI SDK
    using Azure.Identity;
    using OpenAI;
    using OpenAI.Chat;
    using System.ClientModel.Primitives;

    #pragma warning disable OPENAI001

    BearerTokenPolicy tokenPolicy = new(
        new DefaultAzureCredential(),
        "https://cognitiveservices.azure.com/.default"
    );

    ChatClient client = new(
        model: "gpt-4o-mini", // Your deployment name
        authenticationPolicy: tokenPolicy,
        options: new OpenAIClientOptions() {
            Endpoint = new Uri("https://<resource>.openai.azure.com/openai/v1/")
        }
    );
    ```

```csharp Azure AI Inference SDK
    using Azure;
    using Azure.Identity;
    using Azure.AI.Inference;

    ChatCompletionsClient client = new ChatCompletionsClient(
        new Uri("https://<resource>.services.ai.azure.com/models"),
        new DefaultAzureCredential()
    );
    ```
</CodeGroup>

## Chat completions

<CodeGroup>
```csharp OpenAI SDK
    using OpenAI.Chat;

    ChatCompletion completion = client.CompleteChat(
        new SystemChatMessage("You are a helpful assistant."),
        new UserChatMessage("What is Azure AI?")
    );

    Console.WriteLine(completion.Content[0].Text);
    ```

```csharp Azure AI Inference SDK
    using Azure.AI.Inference;

    ChatCompletionsOptions requestOptions = new ChatCompletionsOptions()
    {
        Messages = {
            new ChatRequestSystemMessage("You are a helpful assistant."),
            new ChatRequestUserMessage("What is Azure AI?")
        },
        Model = "DeepSeek-V3.1", // Optional for single-model endpoints
    };

    Response<ChatCompletions> response = client.Complete(requestOptions);
    Console.WriteLine(response.Value.Choices[0].Message.Content);
    ```
</CodeGroup>

### Streaming

<CodeGroup>
```csharp OpenAI SDK
    using OpenAI.Chat;

    CollectionResult<StreamingChatCompletionUpdate> updates = client.CompleteChatStreaming(
        new SystemChatMessage("You are a helpful assistant."),
        new UserChatMessage("Write a poem about Azure.")
    );

    foreach (StreamingChatCompletionUpdate update in updates)
    {
        foreach (ChatMessageContentPart part in update.ContentUpdate)
        {
            Console.Write(part.Text);
        }
    }
    ```

```csharp Azure AI Inference SDK
    using Azure.AI.Inference;

    ChatCompletionsOptions requestOptions = new ChatCompletionsOptions()
    {
        Messages = {
            new ChatRequestSystemMessage("You are a helpful assistant."),
            new ChatRequestUserMessage("Write a poem about Azure.")
        },
        Model = "gpt-4o-mini",
    };

    StreamingResponse<StreamingChatCompletionsUpdate> response = client.CompleteStreaming(requestOptions);

    await foreach (StreamingChatCompletionsUpdate update in response)
    {
        if (update.ContentUpdate != null)
        {
            Console.Write(update.ContentUpdate);
        }
    }
    ```
</CodeGroup>

## Embeddings

<CodeGroup>
```csharp OpenAI SDK
    using OpenAI;
    using OpenAI.Embeddings;
    using System.ClientModel;

    EmbeddingClient client = new(
        "text-embedding-3-small",
        credential: new ApiKeyCredential("API-KEY"),
        options: new OpenAIClientOptions()
        {

            Endpoint = new Uri("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1")
        }
    );

    string input = "This is a test";

    OpenAIEmbedding embedding = client.GenerateEmbedding(input);
    ReadOnlyMemory<float> vector = embedding.ToFloats();
    Console.WriteLine($"Embeddings: [{string.Join(", ", vector.ToArray())}]");
    ```

```csharp Azure AI Inference SDK
    using Azure;
    using Azure.AI.Inference;

    EmbeddingsClient client = new EmbeddingsClient(
        new Uri("https://<resource>.services.ai.azure.com/models"),
        new AzureKeyCredential(Environment.GetEnvironmentVariable("AZURE_INFERENCE_CREDENTIAL"))
    );

    EmbeddingsOptions embeddingsOptions = new EmbeddingsOptions()
    {
        Input = { "Your text string goes here" },
        Model = "text-embedding-3-small"
    };

    Response<EmbeddingsResult> response = client.Embed(embeddingsOptions);
    ReadOnlyMemory<float> embedding = response.Value.Data[0].Embedding;
    ```
</CodeGroup>

## Setup

Install the OpenAI SDK:

```bash
npm install openai
```

For Microsoft Entra ID authentication, also install:

```bash
npm install @azure/identity
```

## Client configuration

With API key authentication:

<CodeGroup>
```javascript OpenAI SDK
    import { OpenAI } from "openai";

    const client = new OpenAI({
        baseURL: "https://<resource>.openai.azure.com/openai/v1/",
        apiKey: process.env.AZURE_OPENAI_API_KEY
    });
    ```

```javascript Azure AI Inference SDK
    import ModelClient from "@azure-rest/ai-inference";
    import { AzureKeyCredential } from "@azure/core-auth";

    const client = ModelClient(
        "https://<resource>.services.ai.azure.com/models", 
        new AzureKeyCredential(process.env.AZURE_INFERENCE_CREDENTIAL)
    );
    ```
</CodeGroup>

With Microsoft Entra ID authentication:

<CodeGroup>
```javascript OpenAI SDK
    import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
    import { OpenAI } from "openai";

    const tokenProvider = getBearerTokenProvider(
        new DefaultAzureCredential(),
        'https://cognitiveservices.azure.com/.default'
    );

    const client = new OpenAI({
        baseURL: "https://<resource>.openai.azure.com/openai/v1/",
        apiKey: tokenProvider
    });
    ```

```javascript Azure AI Inference SDK
    import ModelClient from "@azure-rest/ai-inference";
    import { DefaultAzureCredential } from "@azure/identity";

    const clientOptions = { 
        credentials: { 
            scopes: ["https://cognitiveservices.azure.com/.default"] 
        } 
    };

    const client = ModelClient(
        "https://<resource>.services.ai.azure.com/models", 
        new DefaultAzureCredential(),
        clientOptions
    );
    ```
</CodeGroup>

## Chat completions

<CodeGroup>
```javascript OpenAI SDK
    const completion = await client.chat.completions.create({
        model: "DeepSeek-V3.1", // Required: your deployment name
        messages: [
            { role: "system", content: "You are a helpful assistant." },
            { role: "user", content: "What is Azure AI?" }
        ]
    });

    console.log(completion.choices[0].message.content);
    ```

```javascript Azure AI Inference SDK
    const response = await client.path("/chat/completions").post({
        body: {
            messages: [
                { role: "system", content: "You are a helpful assistant." },
                { role: "user", content: "What is Azure AI?" }
            ],
            model: "DeepSeek-V3.1" // Optional for single-model endpoints
        }
    });

    console.log(response.body.choices[0].message.content);
    ```
</CodeGroup>

### Streaming

<CodeGroup>
```javascript OpenAI SDK
    const stream = await client.chat.completions.create({
        model: "DeepSeek-V3.1",
        messages: [
            { role: "system", content: "You are a helpful assistant." },
            { role: "user", content: "Write a poem about Azure." }
        ],
        stream: true
    });

    for await (const chunk of stream) {
        if (chunk.choices[0]?.delta?.content) {
            process.stdout.write(chunk.choices[0].delta.content);
        }
    }
    ```

```javascript Azure AI Inference SDK
    const response = await client.path("/chat/completions").post({
        body: {
            messages: [
                { role: "system", content: "You are a helpful assistant." },
                { role: "user", content: "Write a poem about Azure." }
            ],
            model: "DeepSeek-V3.1",
            stream: true
        }
    }).asNodeStream();

    for await (const chunk of response) {
        if (chunk.choices && chunk.choices[0]?.delta?.content) {
            process.stdout.write(chunk.choices[0].delta.content);
        }
    }
    ```
</CodeGroup>

## Embeddings

<CodeGroup>
```javascript OpenAI SDK
    import OpenAI from "openai";
    import { getBearerTokenProvider, DefaultAzureCredential } from "@azure/identity";

    const tokenProvider = getBearerTokenProvider(
        new DefaultAzureCredential(),
        'https://cognitiveservices.azure.com/.default');
    const client = new OpenAI({
        baseURL: "https://<resource>.openai.azure.com/openai/v1/",
        apiKey: tokenProvider
    });

    const embedding = await client.embeddings.create({
      model: "text-embedding-3-large", // Required: your deployment name
      input: "The quick brown fox jumped over the lazy dog",
      encoding_format: "float",
    });

    console.log(embedding);
    ```

```javascript Azure AI Inference SDK
    import ModelClient from "@azure-rest/ai-inference";
    import { AzureKeyCredential } from "@azure/core-auth";

    const client = ModelClient(
        "https://<resource>.services.ai.azure.com/models",
        new AzureKeyCredential(process.env.AZURE_INFERENCE_CREDENTIAL)
    );

    const response = await client.path("/embeddings").post({
        body: {
            input: ["Your text string goes here"],
            model: "text-embedding-3-small"
        }
    });

    const embedding = response.body.data[0].embedding;
    ```
</CodeGroup>

## Setup

Add the OpenAI SDK to your project. Check the [OpenAI Java GitHub repository](https://github.com/openai/openai-java) for the latest version and installation instructions.

For Microsoft Entra ID authentication, also add:

```xml
<dependency>
    <groupId>com.azure</groupId>
    <artifactId>azure-identity</artifactId>
    <version>1.18.0</version>
</dependency>
```

## Client configuration

With API key authentication:

<CodeGroup>
```java OpenAI SDK
    import com.openai.client.OpenAIClient;
    import com.openai.client.okhttp.OpenAIOkHttpClient;

    OpenAIClient client = OpenAIOkHttpClient.builder()
        .baseUrl("https://<resource>.openai.azure.com/openai/v1/")
        .apiKey(System.getenv("AZURE_OPENAI_API_KEY"))
        .build();
    ```

```java Azure AI Inference SDK
    import com.azure.ai.inference.ChatCompletionsClient;
    import com.azure.ai.inference.ChatCompletionsClientBuilder;
    import com.azure.core.credential.AzureKeyCredential;

    ChatCompletionsClient client = new ChatCompletionsClientBuilder()
        .credential(new AzureKeyCredential(System.getenv("AZURE_INFERENCE_CREDENTIAL")))
        .endpoint("https://<resource>.services.ai.azure.com/models")
        .buildClient();
    ```
</CodeGroup>

With Microsoft Entra ID authentication:

<CodeGroup>
```java OpenAI SDK
    import com.openai.client.OpenAIClient;
    import com.openai.client.okhttp.OpenAIOkHttpClient;
    import com.azure.identity.DefaultAzureCredential;
    import com.azure.identity.DefaultAzureCredentialBuilder;

    DefaultAzureCredential tokenCredential = new DefaultAzureCredentialBuilder().build();

    OpenAIClient client = OpenAIOkHttpClient.builder()
        .baseUrl("https://<resource>.openai.azure.com/openai/v1/")
        .credential(BearerTokenCredential.create(
            AuthenticationUtil.getBearerTokenSupplier(
                tokenCredential, 
                "https://cognitiveservices.azure.com/.default"
            )
        ))
        .build();
    ```

```java Azure AI Inference SDK
    import com.azure.ai.inference.ChatCompletionsClient;
    import com.azure.ai.inference.ChatCompletionsClientBuilder;
    import com.azure.identity.DefaultAzureCredential;
    import com.azure.identity.DefaultAzureCredentialBuilder;
    import com.azure.core.credential.TokenCredential;

    TokenCredential credential = new DefaultAzureCredentialBuilder().build();
    ChatCompletionsClient client = new ChatCompletionsClientBuilder()
        .credential(credential)
        .endpoint("https://<resource>.services.ai.azure.com/models")
        .buildClient();
    ```
</CodeGroup>

## Chat completions

<CodeGroup>
```java OpenAI SDK
    import com.openai.models.chat.completions.*;

    ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
        .addSystemMessage("You are a helpful assistant.")
        .addUserMessage("What is Azure AI?")
        .model("DeepSeek-V3.1") // Required: your deployment name
        .build();

    ChatCompletion completion = client.chat().completions().create(params);
    System.out.println(completion.choices().get(0).message().content());
    ```

```java Azure AI Inference SDK
    import com.azure.ai.inference.models.*;
    import java.util.List;

    List<ChatRequestMessage> messages = List.of(
        new ChatRequestSystemMessage("You are a helpful assistant."),
        new ChatRequestUserMessage("What is Azure AI?")
    );

    ChatCompletionsOptions options = new ChatCompletionsOptions(messages);
    options.setModel("DeepSeek-V3.1"); // Optional for single-model endpoints

    ChatCompletions response = client.complete(options);
    System.out.println(response.getChoices().get(0).getMessage().getContent());
    ```
</CodeGroup>

### Streaming

<CodeGroup>
```java OpenAI SDK
    import com.openai.models.chat.completions.*;
    import java.util.stream.Stream;

    ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
        .addSystemMessage("You are a helpful assistant.")
        .addUserMessage("Write a poem about Azure.")
        .model("DeepSeek-V3.1") // Required: your deployment name
        .build();

    Stream<ChatCompletionChunk> stream = client.chat().completions().createStreaming(params);

    stream.forEach(chunk -> {
        if (chunk.choices() != null && !chunk.choices().isEmpty()) {
            String content = chunk.choices().get(0).delta().content();
            if (content != null) {
                System.out.print(content);
            }
        }
    });
    ```

```java Azure AI Inference SDK
    import com.azure.ai.inference.models.*;

    List<ChatRequestMessage> messages = List.of(
        new ChatRequestSystemMessage("You are a helpful assistant."),
        new ChatRequestUserMessage("Write a poem about Azure.")
    );

    ChatCompletionsOptions options = new ChatCompletionsOptions(messages);
    options.setModel("DeepSeek-V3.1");

    IterableStream<ChatCompletions> response = client.completeStream(options);

    response.forEach(update -> {
        if (update.getChoices() != null && !update.getChoices().isEmpty()) {
            String content = update.getChoices().get(0).getDelta().getContent();
            if (content != null) {
                System.out.print(content);
            }
        }
    });
    ```
</CodeGroup>

## Embeddings

<CodeGroup>
```java OpenAI SDK
    package com.openai.example;

    import com.openai.client.OpenAIClient;
    import com.openai.client.okhttp.OpenAIOkHttpClient;
    import com.openai.models.embeddings.EmbeddingCreateParams;
    import com.openai.models.embeddings.EmbeddingModel;

    public final class EmbeddingsExample {
        private EmbeddingsExample() {}

        public static void main(String[] args) {
            // Configures using one of:
            // - The `OPENAI_API_KEY` environment variable
            // - The `OPENAI_BASE_URL` and `AZURE_OPENAI_KEY` environment variables
            OpenAIClient client = OpenAIOkHttpClient.fromEnv();

            EmbeddingCreateParams createParams = EmbeddingCreateParams.builder()
                    .input("The quick brown fox jumped over the lazy dog")
                    .model(EmbeddingModel.TEXT_EMBEDDING_3_SMALL)
                    .build();

            System.out.println(client.embeddings().create(createParams));
        }
    }
    ```

```java Azure AI Inference SDK
    import com.azure.ai.inference.EmbeddingsClient;
    import com.azure.ai.inference.EmbeddingsClientBuilder;
    import com.azure.core.credential.AzureKeyCredential;

    EmbeddingsClient client = new EmbeddingsClientBuilder()
        .credential(new AzureKeyCredential(System.getenv("AZURE_INFERENCE_CREDENTIAL")))
        .endpoint("https://<resource>.services.ai.azure.com/models")
        .buildClient();

    EmbeddingsOptions embeddingsOptions = new EmbeddingsOptions(
        List.of("Your text string goes here")
    );
    embeddingsOptions.setModel("text-embedding-3-small");

    EmbeddingsResult response = client.embed(embeddingsOptions);
    List<Float> embedding = response.getData().get(0).getEmbedding();
    ```
</CodeGroup>

## Setup

Install the OpenAI SDK:

```bash
go get github.com/openai/openai-go/v3
```

For Microsoft Entra ID authentication, also install:

```bash
go get -u github.com/Azure/azure-sdk-for-go/sdk/azidentity
```

## Client configuration

With API key authentication:

<Tabs>
  <Tab title="OpenAI SDK">

    ```go
    import (
        "github.com/openai/openai-go/v3"
        "github.com/openai/openai-go/v3/option"
    )

    client := openai.NewClient(
        option.WithBaseURL("https://<resource>.openai.azure.com/openai/v1/"),
        option.WithAPIKey(os.Getenv("AZURE_OPENAI_API_KEY")),
    )
    ```

  </Tab>
  <Tab title="Azure AI Inference SDK">

    Azure AI Inference SDK for Go uses Azure SDK patterns.

  </Tab>
</Tabs>

With Microsoft Entra ID authentication:

<Tabs>
  <Tab title="OpenAI SDK">

    ```go
    import (
        "github.com/Azure/azure-sdk-for-go/sdk/azidentity"
        "github.com/openai/openai-go/v3"
        "github.com/openai/openai-go/v3/azure"
        "github.com/openai/openai-go/v3/option"
    )

    tokenCredential, err := azidentity.NewDefaultAzureCredential(nil)
    if err != nil {
        panic(err)
    }

    client := openai.NewClient(
        option.WithBaseURL("https://<resource>.openai.azure.com/openai/v1/"),
        azure.WithTokenCredential(tokenCredential),
    )
    ```

  </Tab>
  <Tab title="Azure AI Inference SDK">

    Azure AI Inference SDK for Go supports Microsoft Entra ID through Azure SDK.

  </Tab>
</Tabs>

## Chat completions

<Tabs>
  <Tab title="OpenAI SDK">

    ```go
    import (
        "context"
        "fmt"
        "github.com/openai/openai-go/v3"
    )

    chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
        Messages: []openai.ChatCompletionMessageParamUnion{
            openai.SystemMessage("You are a helpful assistant."),
            openai.UserMessage("What is Azure AI?"),
        },
        Model: "DeepSeek-V3.1", // Required: your deployment name
    })

    if err != nil {
        panic(err.Error())
    }

    fmt.Println(chatCompletion.Choices[0].Message.Content)
    ```

  </Tab>
  <Tab title="Azure AI Inference SDK">

    Azure AI Inference SDK for Go uses Azure SDK patterns for chat completions.

  </Tab>
</Tabs>

### Streaming

<Tabs>
  <Tab title="OpenAI SDK">

    ```go
    import (
        "context"
        "fmt"
        "github.com/openai/openai-go/v3"
    )

    stream := client.Chat.Completions.NewStreaming(context.TODO(), openai.ChatCompletionNewParams{
        Messages: []openai.ChatCompletionMessageParamUnion{
            openai.SystemMessage("You are a helpful assistant."),
            openai.UserMessage("Write a poem about Azure."),
        },
        Model: "DeepSeek-V3.1", // Required: your deployment name
    })

    for stream.Next() {
        chunk := stream.Current()
        if len(chunk.Choices) > 0 && chunk.Choices[0].Delta.Content != "" {
            fmt.Print(chunk.Choices[0].Delta.Content)
        }
    }

    if err := stream.Err(); err != nil {
        panic(err.Error())
    }
    ```

  </Tab>
  <Tab title="Azure AI Inference SDK">

    Azure AI Inference SDK for Go supports streaming through Azure SDK patterns.

  </Tab>
</Tabs>

## Embeddings

<Tabs>
  <Tab title="OpenAI SDK">

    ```go
    package main

    import (
        "context"
        "fmt"
        "log"

        "github.com/Azure/azure-sdk-for-go/sdk/azidentity"
        "github.com/openai/openai-go/v3"
        "github.com/openai/openai-go/v3/azure"
        "github.com/openai/openai-go/v3/option"
    )

    func main() {
        tokenCredential, err := azidentity.NewDefaultAzureCredential(nil)
        if err != nil {
            log.Fatalf("Error creating credential:%s", err)
        }
        // Create a client with Azure OpenAI endpoint and Entra ID credentials
        client := openai.NewClient(
            option.WithBaseURL("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"),
            azure.WithTokenCredential(tokenCredential),
        )

        inputText := "The quick brown fox jumped over the lazy dog"

        // Make the embedding request synchronously
        resp, err := client.Embeddings.New(context.Background(), openai.EmbeddingNewParams{
            Model: openai.EmbeddingModel("text-embedding-3-large"), // Use your deployed model name on Azure
            Input: openai.EmbeddingNewParamsInputUnion{
                OfArrayOfStrings: []string{inputText},
            },
        })
        if err != nil {
            log.Fatalf("Failed to get embedding: %s", err)
        }

        if len(resp.Data) == 0 {
            log.Fatalf("No embedding data returned.")
        }

        // Print embedding information
        embedding := resp.Data[0].Embedding
        fmt.Printf("Embedding Length: %d\n", len(embedding))
        fmt.Println("Embedding Values:")
        for _, value := range embedding {
            fmt.Printf("%f, ", value)
        }
        fmt.Println()
    }

    ```

  </Tab>
  <Tab title="Azure AI Inference SDK">

    Azure AI Inference SDK for Go uses Azure SDK patterns for embeddings.

  </Tab>
</Tabs>

## Common migration patterns

### Model parameter handling

- **Azure AI Inference SDK**: The `model` parameter is optional for single-model endpoints but required for multimodel endpoints.
- **OpenAI SDK**: The `model` parameter is always required and should be set to your deployment name.

### Endpoint URL format

- **Azure AI Inference SDK**: Uses `https://<resource>.services.ai.azure.com/models`.
- **OpenAI SDK**: Uses `https://<resource>.openai.azure.com/openai/v1` (connects to the OpenAI v1 API).

### Response structure

The response structure is similar but has some differences:

- **Azure AI Inference SDK**: Returns `ChatCompletions` object with `choices[].message.content`.
- **OpenAI SDK**: Returns `ChatCompletion` object with `choices[].message.content`.

Both SDKs provide similar access patterns to response data, including:
- Message content
- Token usage
- Model information
- Finish reason

## Migration checklist

Use this checklist to ensure a smooth migration:

<Check>Install the OpenAI SDK for your programming language</Check>
<Check>Update authentication code (API key or Microsoft Entra ID)</Check>
<Check>Change endpoint URLs from `.services.ai.azure.com/models` to `.openai.azure.com/openai/v1/`</Check>
<Check>Update client initialization code</Check>
<Check>Always specify the `model` parameter with your deployment name</Check>
<Check>Update request method calls (`complete` â†’ `chat.completions.create`)</Check>
<Check>Update streaming code if applicable</Check>
<Check>Update error handling to use OpenAI SDK exceptions</Check>
<Check>Test all functionality thoroughly</Check>
<Check>Update documentation and code comments</Check>
## Troubleshooting

### Authentication failures

If you experience authentication failures:

- Verify your API key is correct and isn't expired
- For Microsoft Entra ID, ensure your application has the correct permissions
- Check that the credential scope is set to `https://cognitiveservices.azure.com/.default`

### Endpoint errors

If you receive endpoint errors:

- Verify the endpoint URL format includes `/openai/v1/` at the end.
- Ensure your resource name is correct.
- Check that the model deployment exists and is active.

### Model not found errors

If you receive "model not found" errors:

- Verify you're using your deployment name, not the model name.
- Check that the deployment is active in your Microsoft Foundry resource.
- Ensure the deployment name matches exactly (case-sensitive).

## Related content

- [Azure OpenAI supported programming languages](/models/capabilities/supported-languages)
- [How to generate chat completions with Foundry Models](/api-sdk/api-version-lifecycle)
- [API evolution and version lifecycle](/api-sdk/api-version-lifecycle)
- [Switch between OpenAI and Azure OpenAI endpoints](https://learn.microsoft.com/azure/developer/ai/how-to/switching-endpoints)
